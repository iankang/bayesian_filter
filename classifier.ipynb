{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation for Bayesian Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install dependencies...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.7.0-cp37-cp37m-win_amd64.whl (157 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\neyer\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (45.2.0.post20200210)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are importing all the libraries we would like to use.\n",
    "#including data stores for stopwords and such\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function reads the files in the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    #this function is reading the actual email files.\n",
    "    #the function will cycle through all the files in the folder specified.\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(root,filename)\n",
    "            \n",
    "            inBody = False\n",
    "            lines = []\n",
    "            #this uses IO functions to read the files by specifying the encoding type\n",
    "            f = io.open(path, 'r', encoding = 'latin1')\n",
    "            #this cycles through each line and fetches the text therein.\n",
    "            for line in f:\n",
    "                if inBody:\n",
    "                    lines.append(line)\n",
    "                elif line == '\\n':\n",
    "                    inBody = True\n",
    "            f.close()\n",
    "            #join the text into a single message.\n",
    "            message = '\\n'.join(lines)\n",
    "            #return the message\n",
    "            yield path, message\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is for creating dataframes from the dataset in question... it also classifies the emails into either ham or spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this classifies the emails into spam or ham depending on predetermined status.\n",
    "def dataFrameFromDirectory(path, classification):\n",
    "    #instantiate the structure of the dataframe needed.\n",
    "    rows = []\n",
    "    index = []\n",
    "    #store the message alongside its classification.\n",
    "    for filename, message in read_files(path):\n",
    "        rows.append({'message': message, 'label': classification})\n",
    "        index.append(filename)\n",
    "        #return the dataframe to be used for manipulation\n",
    "    return pd.DataFrame(rows, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate all the emails into a single dataframe. The key is: ham = 0, spam = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [message, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch the data from the source files.\n",
    "#here you run the datasets sequentially for validation by specifying file name. \n",
    "\n",
    "data = pd.DataFrame({'message':[],'label':[]})\n",
    "data = data.append(dataFrameFromDirectory(\"datasets/ham3\",0))\n",
    "data = data.append(dataFrameFromDirectory(\"datasets/spam3\",1))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell shows the number of rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_mails = data['message'].shape[0]\n",
    "total_mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below extracts all text from html text present in all emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for scraping the emails, since most are in the form of html based content.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def func(df):\n",
    "    soup = BeautifulSoup(df['message'], \"html.parser\").find()\n",
    "    #check emails with html syntax\n",
    "    if bool(soup):\n",
    "        soup = BeautifulSoup(df['message'], \"html.parser\")\n",
    "        #extract text only from the whole email.\n",
    "    \n",
    "        text = soup.find_all(text=True)\n",
    "      \n",
    "        #return all the words found.\n",
    "        text = ''.join(word for word in  text)\n",
    "        df['message'] = text\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        \n",
    "        return df['message'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, message, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = data.reset_index()\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-aacbf6d7f0ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#this removes all text that is not alphanumeric i.e special characters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5268\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5269\u001b[0m         ):\n\u001b[1;32m-> 5270\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2038\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2039\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2040\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"string\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   2094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "#this removes all text that is not alphanumeric i.e special characters.\n",
    "new_data[new_data['message'].str.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data into the pareto principle for unbiased model testing.\n",
    "so 80% will be used for training and 20% for testing.\n",
    "this will be accomplished by using a random number generator to randomize the order by implementing a uniform distribution randomizer. this removes all chances of having a biased model on account of sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and testing sets\n",
    "#initializing empty lists to hold training and testing data respectively\n",
    "train_index, test_index = list(), list()\n",
    "for i in range(data['message'].shape[0]):\n",
    "    #make use of the uniform random distribution to alleviate contiguousness of pseudo-random number generators.\n",
    "    #80% for training and 20% for testing according to the pareto principle.\n",
    "    if np.random.uniform(0,1) < 0.80:\n",
    "        train_index.append(i)\n",
    "    else:\n",
    "        test_index.append(i)\n",
    "#training data dataframe.\n",
    "train_data = new_data.loc[train_index]\n",
    "#test data dataframe\n",
    "test_data = new_data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement the data cleaning of the html based messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we  apply the scraping function to our data. hence cleaning it.\n",
    "train_data['message'] = train_data.apply(func, axis=1)\n",
    "test_data['message'] = test_data.apply(func, axis=1)\n",
    "train_data #its just expecting the data to come from a website and not a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression. it is used to perform word searches.\n",
    "\n",
    "train_data['message'] = train_data['message'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "test_data['message'] = test_data['message'].map(lambda x: re.sub(r'\\W+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['message'].str.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visual representation of the most salient words.\n",
    "#this is the visulization for the spam.\n",
    "spam_words = ' '.join(str(moja) for moja in list(test_data[test_data['label'] == 1]['message']))\n",
    "spamwc = WordCloud(width = 512, height=512).generate(spam_words)\n",
    "plt.figure(figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spamwc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the ham data\n",
    "spam_words = ' '.join(str(moja) for moja in list(test_data[test_data['label'] == 0]['message']))\n",
    "spamwc = WordCloud(width = 512, height=512).generate(spam_words)\n",
    "plt.figure(figsize=(10,8), facecolor='k')\n",
    "plt.imshow(spamwc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label'].value_counts() #remove the paragraph line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and processing the words harnessed.\n",
    "def process_message(message, lower_case = True, stem = True, stop_words = True, gram = 2):\n",
    "    #lower case all the letters in the message.\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    #break down the words into tokens.\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "#     print(words)\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        #fetch all words that do not have meaning in this case: stopwords.\n",
    "        sw = stopwords.words('english')\n",
    "        #create a list of words containing only the words without stopwords\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]   \n",
    "#     print(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this whole function is the bayesian classifier. the calc_prob calculates the probability of a word bein either spam or ham.\n",
    "#the classify function returns true if the threshold for it being spam has been reached. otherwise, it is false, meaning ham.\n",
    "class SpamClassifier(object):\n",
    "    def __init__(self, train_data, method = 'tf-idf'):\n",
    "        #initialize the data to be used for processing. i.e training data.\n",
    "        self.mails, self.labels = train_data['message'], train_data['label']\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        #this function trains our model.\n",
    "        self.calc_TF_and_IDF()\n",
    "        if self.method == 'tf-idf':\n",
    "            self.calc_TF_IDF()\n",
    "        else:\n",
    "            self.calc_prob()\n",
    "\n",
    "    def calc_prob(self):\n",
    "        # initialize variables to calculate probabilities. i.e probability of being ham or spam. using normal probabilistic theory.\n",
    "        self.prob_spam = dict()\n",
    "        self.prob_ham = dict()\n",
    "        for word in self.tf_spam:\n",
    "            #if word is spam create a dictionary with the key being the word and the value being the probability of it being spam.\n",
    "            self.prob_spam[word] = (self.tf_spam[word] + 1) / (self.spam_words + \\\n",
    "                                                                len(list(self.tf_spam.keys())))\n",
    "        for word in self.tf_ham:\n",
    "            #if word is ham, create a dictionary with the key being the ham word and the value being its probability of being ham.\n",
    "            self.prob_ham[word] = (self.tf_ham[word] + 1) / (self.ham_words + \\\n",
    "                                                                len(list(self.tf_ham.keys())))\n",
    "        self.prob_spam_mail, self.prob_ham_mail = self.spam_mails / self.total_mails, self.ham_mails / self.total_mails \n",
    "\n",
    "\n",
    "    def calc_TF_and_IDF(self):\n",
    "        noOfMessages = self.mails.shape[0]\n",
    "        self.spam_mails, self.ham_mails = self.labels.value_counts()[1], self.labels.value_counts()[0]\n",
    "        self.total_mails = self.spam_mails + self.ham_mails\n",
    "        self.spam_words = 0\n",
    "        self.ham_words = 0\n",
    "        self.tf_spam = dict()\n",
    "        self.tf_ham = dict()\n",
    "        self.idf_spam = dict()\n",
    "        self.idf_ham = dict()\n",
    "        self.missing_index = [number for number in range(0,noOfMessages) if number not in self.mails.index ]\n",
    "        for i in range(noOfMessages):\n",
    "            if i not in self.missing_index:\n",
    "                message_processed = process_message(self.mails[i])\n",
    "                count = list() #To keep track of whether the word has ocured in the message or not.\n",
    "                               #For IDF\n",
    "                for word in message_processed:\n",
    "                    if self.labels[i]:\n",
    "                        self.tf_spam[word] = self.tf_spam.get(word, 0) + 1\n",
    "                        self.spam_words += 1\n",
    "                    else:\n",
    "                        self.tf_ham[word] = self.tf_ham.get(word, 0) + 1\n",
    "                        self.ham_words += 1\n",
    "                    if word not in count:\n",
    "                        count += [word]\n",
    "                for word in count:\n",
    "                    if self.labels[i]:\n",
    "                        self.idf_spam[word] = self.idf_spam.get(word, 0) + 1\n",
    "                    else:\n",
    "                        self.idf_ham[word] = self.idf_ham.get(word, 0) + 1\n",
    "\n",
    "    def calc_TF_IDF(self):\n",
    "        self.prob_spam = dict()\n",
    "        self.prob_ham = dict()\n",
    "        self.sum_tf_idf_spam = 0\n",
    "        self.sum_tf_idf_ham = 0\n",
    "        for word in self.tf_spam:\n",
    "            self.prob_spam[word] = (self.tf_spam[word]) * log((self.spam_mails + self.ham_mails) \\\n",
    "                                                          / (self.idf_spam[word] + self.idf_ham.get(word, 0)))\n",
    "            self.sum_tf_idf_spam += self.prob_spam[word]\n",
    "        for word in self.tf_spam:\n",
    "            self.prob_spam[word] = (self.prob_spam[word] + 1) / (self.sum_tf_idf_spam + len(list(self.prob_spam.keys())))\n",
    "            \n",
    "        for word in self.tf_ham:\n",
    "            self.prob_ham[word] = (self.tf_ham[word]) * log((self.spam_mails + self.ham_mails) \\\n",
    "                                                          / (self.idf_spam.get(word, 0) + self.idf_ham[word]))\n",
    "            self.sum_tf_idf_ham += self.prob_ham[word]\n",
    "        for word in self.tf_ham:\n",
    "            self.prob_ham[word] = (self.prob_ham[word] + 1) / (self.sum_tf_idf_ham + len(list(self.prob_ham.keys())))\n",
    "            \n",
    "    \n",
    "        self.prob_spam_mail, self.prob_ham_mail = self.spam_mails / self.total_mails, self.ham_mails / self.total_mails \n",
    "       #the method that does the actual classifying.             \n",
    "    def classify(self, processed_message):\n",
    "        pSpam, pHam = 0, 0\n",
    "        for word in processed_message:                \n",
    "            if word in self.prob_spam:\n",
    "                pSpam += log(self.prob_spam[word])\n",
    "            else:\n",
    "                try:\n",
    "                    if self.method == 'tf-idf':\n",
    "                        pSpam -= log(self.sum_tf_idf_spam + len(list(self.prob_spam.keys())))\n",
    "                    else:\n",
    "                        pSpam -= log(self.spam_words + len(list(self.prob_spam.keys())))\n",
    "                except:\n",
    "                    print('oops')\n",
    "            if word in self.prob_ham:\n",
    "                pHam += log(self.prob_ham[word])\n",
    "            else:\n",
    "                try:\n",
    "                    if self.method == 'tf-idf':\n",
    "                        pHam -= log(self.sum_tf_idf_ham + len(list(self.prob_ham.keys()))) \n",
    "                    else:\n",
    "                        pHam -= log(self.ham_words + len(list(self.prob_ham.keys())))\n",
    "                except:\n",
    "                    print('oops')\n",
    "            pSpam += log(self.prob_spam_mail)\n",
    "            pHam += log(self.prob_ham_mail)\n",
    "            #if probability of spam is higher, it returns true.\n",
    "            return pSpam >= pHam\n",
    "    \n",
    "    def predict(self, testData):\n",
    "        result = dict()\n",
    "        for (i, message) in enumerate(testData):\n",
    "            processed_message = process_message(message)\n",
    "        \n",
    "            result[i] = int(self.classify(processed_message))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(labels, predictions): #Confusion matrix function\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        true_pos += int(labels[i] == 1 and predictions[i] == 1)\n",
    "        true_neg += int(labels[i] == 0 and predictions[i] == 0)\n",
    "        false_pos += int(labels[i] == 0 and predictions[i] == 1)\n",
    "        false_neg += int(labels[i] == 1 and predictions[i] == 0)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    Fscore = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-score: \", Fscore)\n",
    "    print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this checks for the metrics based on the bag of words method\n",
    "sc_bow = SpamClassifier(train_data, 'bow') \n",
    "sc_bow.train()\n",
    "preds_bow = sc_bow.predict(test_data['message'])\n",
    "metrics(test_data['label'], preds_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this checks for the metrics based on tf-idf method\n",
    "sc_tf_idf = SpamClassifier(train_data, 'tf-idf')\n",
    "sc_tf_idf.train()\n",
    "preds_tf_idf = sc_tf_idf.predict(test_data['message'])\n",
    "metrics(test_data['label'], preds_tf_idf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = process_message(\"No, this isn't a reference for the website which you used. Your reference implies that you are paraphrasing information from the actual article which clearly you are not doing.\")\n",
    "if(sc_tf_idf.classify(pm)):\n",
    "    print(\"Message is spam\")\n",
    "else:\n",
    "    print(\"Message is Ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
